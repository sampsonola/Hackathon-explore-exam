{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd69ef0f",
   "metadata": {},
   "source": [
    "### Importing of Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d764c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2022-06-25 22:27:38 +01:00)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c92401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\samson\\appdata\\roaming\\python\\python39\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\samson\\anaconda3\\lib\\site-packages (from imblearn) (0.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\samson\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\samson\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\samson\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\samson\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\samson\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.20.3)\n",
      "time: 6.99 s (started: 2022-06-25 22:27:39 +01:00)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136e39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SAMSON\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SAMSON\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.4 s (started: 2022-06-25 22:27:46 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "# Standard libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download(['stopwords','punkt'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.utils import resample\n",
    "from imblearn.pipeline import Pipeline\n",
    "# Building classification models\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3bab1",
   "metadata": {},
   "source": [
    "### Loading of Dataset and general overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4933e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 578 ms (started: 2022-06-25 22:28:06 +01:00)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc761c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xho</td>\n",
       "      <td>umgaqo-siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xho</td>\n",
       "      <td>i-dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>the province of kwazulu-natal department of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nso</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ven</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_id                                               text\n",
       "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...\n",
       "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...\n",
       "2     eng  the province of kwazulu-natal department of tr...\n",
       "3     nso  o netefatša gore o ba file dilo ka moka tše le...\n",
       "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 93 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and features\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98f6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "=============\n",
      "\n",
      "Shape of the dataset: (33000, 2)\n",
      "\n",
      "Total Number of unique tweets: 29948\n",
      "\n",
      "Total Number of missing values:\n",
      "lang_id    0\n",
      "text       0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "TEST DATA\n",
      "=========\n",
      "\n",
      "Shape of the dataset: (5682, 2)\n",
      "\n",
      "Total Number of unique tweets: 5459\n",
      "\n",
      "Total Number of missing values:\n",
      "index    0\n",
      "text     0\n",
      "dtype: int64\n",
      "\n",
      "time: 94 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "#Taking general overview at both datasets\n",
    "print('TRAINING DATA')\n",
    "print('============='+('\\n'))\n",
    "print('Shape of the dataset: {}\\n'.format(train.shape))\n",
    "print('Total Number of unique tweets: {}\\n'.format(len(set(train['text']))))\n",
    "print('Total Number of missing values:\\n{}\\n\\n'.format(train.isnull().sum()))\n",
    "print('TEST DATA')\n",
    "print('========='+('\\n'))\n",
    "print('Shape of the dataset: {}\\n'.format(test.shape))\n",
    "print('Total Number of unique tweets: {}\\n'.format(len(set(test['text']))))\n",
    "print('Total Number of missing values:\\n{}\\n' .format(test.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f885e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ffeb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    '''\n",
    "    This functions cleans text from line breaks, URLs, numbers, etc.\n",
    "    '''\n",
    "    \n",
    "    text = text.lower() #to lower case\n",
    "    text = text.replace('\\n', ' ') # remove line breaks\n",
    "    text = text.replace('\\@(\\w*)', '') # remove mentions\n",
    "    text = re.sub(r\"\\bhttps://t.co/\\w+\", '', text) # remove URLs\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # remove numbers\n",
    "    text = re.sub(r'\\#', '', text) # remove hashtags. To remove full hashtag: '\\#(\\w*)'\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # removes numbers?\n",
    "    text = re.sub(' +', ' ', text) # remove 1+ spaces\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    text =' '.join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7c4993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xho</td>\n",
       "      <td>umgaqo-siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xho</td>\n",
       "      <td>i-dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>the province of kwazulu-natal department of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nso</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ven</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_id                                               text\n",
       "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...\n",
       "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...\n",
       "2     eng  the province of kwazulu-natal department of tr...\n",
       "3     nso  o netefatša gore o ba file dilo ka moka tše le...\n",
       "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the labels and features\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db40632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mmasepala, fa maemo a a kgethegileng a letlele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Uzakwaziswa ngokufaneleko nakungafuneka eminye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tshivhumbeo tshi fana na ngano dza vhathu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kube inja nelikati betingevakala kutsi titsini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Winste op buitelandse valuta.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text\n",
       "0      1  Mmasepala, fa maemo a a kgethegileng a letlele...\n",
       "1      2  Uzakwaziswa ngokufaneleko nakungafuneka eminye...\n",
       "2      3         Tshivhumbeo tshi fana na ngano dza vhathu.\n",
       "3      4  Kube inja nelikati betingevakala kutsi titsini...\n",
       "4      5                      Winste op buitelandse valuta."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 156 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99898625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    29948\n",
       "True      3052\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 203 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "train.text.duplicated(keep=\"first\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977e967f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5459\n",
       "True      223\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 157 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "test.text.duplicated(keep=\"first\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c51aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated text in train data:\n",
      "9.25 %\n",
      "time: 282 ms (started: 2022-06-25 22:28:07 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Looking for duplicates\n",
    "percent_duplicates = round((1-(train['text'].nunique()/len(train['text'])))*100,2)\n",
    "print('Duplicated text in train data:')\n",
    "print(percent_duplicates,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a5f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29948, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 187 ms (started: 2022-06-25 22:28:08 +01:00)\n"
     ]
    }
   ],
   "source": [
    "train.drop_duplicates(subset=\"text\",keep=\"first\",inplace=True,ignore_index=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c79f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated text in train data:\n",
      "3.92 %\n",
      "time: 125 ms (started: 2022-06-25 22:28:08 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Looking for duplicates\n",
    "percent_duplicates = round((1-(test['text'].nunique()/len(test['text'])))*100,2)\n",
    "print('Duplicated text in train data:')\n",
    "print(percent_duplicates,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e2e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.16 s (started: 2022-06-25 22:28:08 +01:00)\n"
     ]
    }
   ],
   "source": [
    "train['text'] = train['text'].apply(text_preprocessing)\n",
    "test['text'] = test['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e1956ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSON\\AppData\\Local\\Temp/ipykernel_12304/2305434871.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train[\"text\"] = train[\"text\"].str.replace(\".txt\", \" text file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 359 ms (started: 2022-06-25 22:28:14 +01:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSON\\AppData\\Local\\Temp/ipykernel_12304/2305434871.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test[\"text\"] = test[\"text\"].str.replace(\".txt\", \" text file\")\n"
     ]
    }
   ],
   "source": [
    "# Replace '.txt' with 'text file'\n",
    "train[\"text\"] = train[\"text\"].str.replace(\".txt\", \" text file\")\n",
    "test[\"text\"] = test[\"text\"].str.replace(\".txt\", \" text file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecb5ce",
   "metadata": {},
   "source": [
    "Great! We have dealt with all the duplicates in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a29b9804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 781 ms (started: 2022-06-25 22:28:15 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Removing extra spaces\n",
    "train['text']=train['text'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f001745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms (started: 2022-06-25 22:28:15 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Removing extra spaces\n",
    "test['text']=test['text'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edb6a9",
   "metadata": {},
   "source": [
    "###### Create a copy\n",
    "\n",
    "The first step in the preprocessing is to create a copy of the train dataframe for the EDA.In this step we start by determining the length of each text and storing this information in a new column. We then tokenize the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a86cd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2022-06-25 22:31:16 +01:00)\n"
     ]
    }
   ],
   "source": [
    " df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e33a5ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xho</td>\n",
       "      <td>umgaqo-siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "      <td>220</td>\n",
       "      <td>[umgaqo-siseko, wenza, amalungiselelo, kumazik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xho</td>\n",
       "      <td>i-dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "      <td>252</td>\n",
       "      <td>[i-dha, iya, kuba, nobulumko, bokubeka, umsebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>the province of kwazulu-natal department of tr...</td>\n",
       "      <td>264</td>\n",
       "      <td>[the, province, of, kwazulu-natal, department,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nso</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "      <td>217</td>\n",
       "      <td>[o, netefatša, gore, o, ba, file, dilo, ka, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ven</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "      <td>239</td>\n",
       "      <td>[khomishini, ya, ndinganyiso, ya, mbeu, yo, ew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_id                                               text  length  \\\n",
       "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...     220   \n",
       "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...     252   \n",
       "2     eng  the province of kwazulu-natal department of tr...     264   \n",
       "3     nso  o netefatša gore o ba file dilo ka moka tše le...     217   \n",
       "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana...     239   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [umgaqo-siseko, wenza, amalungiselelo, kumazik...  \n",
       "1  [i-dha, iya, kuba, nobulumko, bokubeka, umsebe...  \n",
       "2  [the, province, of, kwazulu-natal, department,...  \n",
       "3  [o, netefatša, gore, o, ba, file, dilo, ka, mo...  \n",
       "4  [khomishini, ya, ndinganyiso, ya, mbeu, yo, ew...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.6 s (started: 2022-06-25 22:32:09 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def lemma(df):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    df['length'] = df['text'].str.len()\n",
    "    df['tokenized'] = df['text'].apply(word_tokenize)\n",
    "    return df\n",
    "\n",
    "df = lemma(df)\n",
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing spacy\n",
    "import spacy\n",
    "\n",
    "# Loading model\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "\n",
    "# Lemmatization with stopwords removal\n",
    "df['lemmatized']=df['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped=df[['text','lemmatized']].groupby(by='text').agg(lambda x:' '.join(x))\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(analyzer='word')\n",
    "data=cv.fit_transform(df_grouped['lemmatized'])\n",
    "df_dtm = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\n",
    "df_dtm.index=df_grouped.index\n",
    "df_dtm.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88673db",
   "metadata": {},
   "source": [
    "## EXPLORATORY data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83472e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(text):\n",
    "    # Count vectorizer excluding english stopwords\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    words = cv.fit_transform(text)\n",
    "    \n",
    "    # Count the words in the texts and determine the frequency of each word\n",
    "    sum_words = words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create a dataframe to store the top 25 words and their frequencies\n",
    "    frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
    "    frequency = frequency.head(25)\n",
    "    \n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
    "#from wordcloud import WordCloud\n",
    "#from textwrap import wrap\n",
    "\n",
    "# Function for generating word clouds\n",
    "#def generate_wordcloud(data,title):\n",
    "# plt.figure(figsize=(10,8))\n",
    "#  plt.imshow(wc, interpolation='bilinear')\n",
    " # plt.axis(\"off\")\n",
    " # plt.title('\\n'.join(wrap(title,60)),fontsize=13)\n",
    "  #plt.show()\n",
    "  # Transposing document term matrix\n",
    "#df_dtm=df_dtm.transpose()\n",
    "\n",
    "# Plotting word cloud for each product\n",
    "#for index,product in enumerate(df_dtm.columns):\n",
    "#generate_wordcloud(df_dtm[product].sort_values(ascending=False),product)\n",
    "# please uncomment it was wasting time and I need to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = set()\n",
    "#corpus= [x.split() for x in df['text'].tolist()]\n",
    "#for sentence in corpus:\n",
    "  for word in sentence:\n",
    " #   vocab.add(word.lower())\n",
    "#print(\"Number of distinct words in raw data: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f43a5",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1c800",
   "metadata": {},
   "source": [
    "### Splitting out X (indepedent) and Y (target/dependent) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['text']\n",
    "y = train['lang_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de9d07",
   "metadata": {},
   "source": [
    "### Splitting of Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1230816",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9873d03",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bb400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [LinearSVC(random_state=42),\n",
    "                SVC(),\n",
    "                #DecisionTreeClassifier(),\n",
    "                #RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                     #random_state=0, class_weight=\"balanced\"),\n",
    "                #MLPClassifier(alpha=1e-5,\n",
    "                             #hidden_layer_sizes=(5, 2),\n",
    "                             #random_state=42),\n",
    "               LogisticRegression(random_state=42,\n",
    "                                  multi_class='ovr',\n",
    "                                  n_jobs=1,\n",
    "                                  C=1e5,\n",
    "                                  max_iter=100000),\n",
    "               KNeighborsClassifier(n_neighbors=5),\n",
    "               MultinomialNB(),\n",
    "               ComplementNB(),\n",
    "               SGDClassifier(loss='hinge',\n",
    "                             penalty='l2',\n",
    "                             alpha=1e-3,\n",
    "                             random_state=42,\n",
    "                             max_iter=5,\n",
    "                             tol=None)\n",
    "                #GradientBoostingClassifier(),\n",
    "                #xgb.XGBClassifier(learning_rate=0.1,\n",
    "                #                 n_estimators=1000,\n",
    "                #                 max_depth=5,\n",
    "                #                min_child_weight=1,\n",
    "                #                 gamma=0,\n",
    "                #                subsample=0.8,\n",
    "                #                 colsample_bytree=0.8,\n",
    "                #                 nthread=4,\n",
    "                #                 seed=27)\n",
    "               ]\n",
    "# The commented code took tool long to load hence the reason for excluding them also, they didn't really perform as much as the ran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a62d06",
   "metadata": {},
   "source": [
    "### Creating Function for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_building(classifiers, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    This function takes in a list of classifiers\n",
    "    and both the train and validation sets\n",
    "    and return a summary of F1-score and\n",
    "    processing time as a dataframe\n",
    "\n",
    "    Input:\n",
    "    classifiers: a list of classifiers to train\n",
    "                 datatype: list\n",
    "    X_train: independent variable for training\n",
    "             datatype: series\n",
    "    y_train: dependent variable for training\n",
    "             datatype: series\n",
    "    X_val: independent variable for validation\n",
    "           datatype: series\n",
    "    y_val: dependent variable for validation\n",
    "           datatype: series\n",
    "\n",
    "    Output:\n",
    "    model_summary: F1 Score for all the classifiers\n",
    "                   datatype: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    models_summary = {}\n",
    "\n",
    "    # Pipeline to balance the classses and then to build the model\n",
    "    for clf in classifiers:\n",
    "        clf_text = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n",
    "                                                       max_df=0.9,\n",
    "                                                       ngram_range=(1, 2))),\n",
    "                             ('clf', clf)])\n",
    "\n",
    "        # Logging the Execution Time for each model\n",
    "        start_time = time.time()\n",
    "        clf_text.fit(X_train, y_train)\n",
    "        predictions = clf_text.predict(X_val)\n",
    "        run_time = time.time()-start_time\n",
    "\n",
    "        # Output for each model\n",
    "        models_summary[clf.__class__.__name__] = {\n",
    "            'F1-Macro': metrics.f1_score(y_val,\n",
    "                                         predictions,\n",
    "                                         average='macro'),\n",
    "            'F1-Accuracy': metrics.f1_score(y_val, predictions,\n",
    "                                            average='micro'),\n",
    "            'F1-Weighted': metrics.f1_score(y_val,\n",
    "                                            predictions,\n",
    "                                            average='weighted'),\n",
    "            'Execution Time': run_time}\n",
    "\n",
    "    return pd.DataFrame.from_dict(models_summary, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca41d64",
   "metadata": {},
   "source": [
    "### Execution of the Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30895822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "classifiers_df = models_building(classifiers, X_train, y_train, X_val, y_val)\n",
    "ordered_df = classifiers_df.sort_values('F1-Macro', ascending=False)\n",
    "ordered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c463e9",
   "metadata": {},
   "source": [
    "### Comparing Classification Methods\n",
    "The most performing is the Multinomial Naive Bayes with F1-Macro of 99.9% and accuracy of 99.9% while closely followed by Complement Naive Bayes, Logistic Regression, Linear Support Vector Classifier, Support Vector Machine etc.\n",
    "\n",
    "Hyperameter tuning is carried out to improve the F1-Macro score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2854e",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning on Most Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refining the train-test split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61324ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Creating a pipeline for the gridsearch\n",
    "param_grid = {'alpha': [0.1, 1, 5, 10]}  # setting parameter grid\n",
    "\n",
    "tuned_mnb = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n",
    "                                                max_df=0.8,\n",
    "                                                ngram_range=(1, 2))),\n",
    "                      ('mnb', GridSearchCV(MultinomialNB(),\n",
    "                                           param_grid=param_grid,\n",
    "                                           cv=5,\n",
    "                                           n_jobs=-1,\n",
    "                                           scoring='f1_weighted'))\n",
    "                      ])\n",
    "\n",
    "tuned_mnb.fit(X_train, y_train)  # Fitting the model\n",
    "\n",
    "y_pred_mnb = tuned_mnb.predict(X_val)  # predicting the fit on validation set\n",
    "\n",
    "print(classification_report(y_val, y_pred_mnb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ab2b8",
   "metadata": {},
   "source": [
    "### Creating File for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_trial = pd.DataFrame(test['index'])\n",
    "submission_trial['lang_id'] = tuned_mnb.predict(test['text'])\n",
    "submission_trial.to_csv('submission_mn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb4459",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Several algorithms were tried and Multinomial Naive Bayes classifier was the most performing. It performed very well on the training and validation datasets with an accuracy score of over 99% and F1 Macro score of over 99%. After testing the fitted model on the held-out/unseen dataset, it was able to predict the classes of languages with an F1 Score of about 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfda49",
   "metadata": {},
   "source": [
    "### References\n",
    "https://colab.research.google.com/drive/1eQydhzxK03z4_20_A9dunOq04trRYuZG?usp=sharing#scrollTo=InfhMWcIHlR_\n",
    "https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
    "https://stackoverflow.com/questions/46864696/how-to-resolve-python-error-while-generating-pie-chart-valueerror-explode-m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9850c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
